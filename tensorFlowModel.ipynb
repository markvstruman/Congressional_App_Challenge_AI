{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving onto the tensorflow classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "      <th>Bias</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>arizona republican gubernatori candid kari lak...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>washington former vice presid mike penc wednes...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>modern republican party’ one nonnegoti valu ru...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>editor’ note elizabeth alexand poet scholar pr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>editor’ note van jone host polit comment found...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2686</th>\n",
       "      <td>amsterdam’ worldfam redlight district could lo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2687</th>\n",
       "      <td>crime risen major citi nationwid recent year m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2688</th>\n",
       "      <td>first nonprofit liber billionair georg soro op...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2689</th>\n",
       "      <td>new listen news articles!twitt ceo elon musk g...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2690</th>\n",
       "      <td>new blow embattl oberlin colleg ohio iranian n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2691 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Article Bias\n",
       "0     arizona republican gubernatori candid kari lak...    0\n",
       "1     washington former vice presid mike penc wednes...    0\n",
       "2     modern republican party’ one nonnegoti valu ru...    0\n",
       "3     editor’ note elizabeth alexand poet scholar pr...    0\n",
       "4     editor’ note van jone host polit comment found...    0\n",
       "...                                                 ...  ...\n",
       "2686  amsterdam’ worldfam redlight district could lo...    1\n",
       "2687  crime risen major citi nationwid recent year m...    1\n",
       "2688  first nonprofit liber billionair georg soro op...    1\n",
       "2689  new listen news articles!twitt ceo elon musk g...    1\n",
       "2690  new blow embattl oberlin colleg ohio iranian n...    1\n",
       "\n",
       "[2691 rows x 2 columns]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "replacements = [\",\", \".\", \":\", \"'\",'\\'', '—', '\"', '\\'' \"!\", \"?\", \".\" \";\", \"@\", \"{\", \"}\", \"[\", \"]\", \"(\", \")\", \"/\", \"#\", \"*\", \"_\", \"%\", \"-\", \"£\", \"$\", \"&\"]\n",
    "articles_prepositions = ['a', 'an', 'the', 'about', 'above', 'across', 'after', 'against', 'among', 'around',\n",
    "'at', 'before', 'behind', 'below', 'beside', 'between', 'by', 'down', 'during', 'for', 'from', 'in', 'inside',\n",
    "'into', 'near', 'of', 'off', 'on', 'out', 'over', 'through', 'to', 'toward', 'under', 'up', 'with']\n",
    "texts = []\n",
    "biases = []\n",
    "data = {}\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "ps = PorterStemmer()\n",
    "for i in range(0,2691):\n",
    "    f = open(\"articles/article%d.txt\" % i, 'r', encoding=\"utf-8\")\n",
    "    contents = f.read().lower()\n",
    "    f.close()\n",
    "\n",
    "    contents = contents.replace(\"cnn\", '')\n",
    "    contents = contents.replace(\"cnn\", '')\n",
    "    contents = contents.replace(\"advertisement\", '')\n",
    "    contents = contents.replace(\"fox\", '')\n",
    "    contents = contents.replace(\"huffpost\", '')\n",
    "    contents = contents.replace(\"washingtonpost\", '')\n",
    "    contents = contents.replace(\"washington post\", '')\n",
    "    contents = contents.replace(\"the federalist\", '')\n",
    "    contents = contents.replace(\"\\n\", '')\n",
    "    for r in range(0, len(replacements)):\n",
    "        contents = contents.replace(replacements[r], '')\n",
    "\n",
    "    for ap in range(0, len(articles_prepositions)):\n",
    "        contents = contents.replace((' ' +articles_prepositions[ap] + ' '), ' ')\n",
    "    \n",
    "    stuff = contents.split()\n",
    "    filtered_stuff = [word for word in stuff if word not in stopwords.words('english')]\n",
    "    contents = ''\n",
    "    for word in filtered_stuff:\n",
    "        word = ps.stem(word)\n",
    "        contents = contents + word + ' '\n",
    "\n",
    "    texts.append(contents)\n",
    "    \n",
    "    g = open(\"articleBiases/article%dBias.txt\" % i, 'r', encoding='utf-8')\n",
    "    bias = g.read()\n",
    "    g.close()\n",
    "    biases.append(bias)\n",
    "\n",
    "data = {'Article': texts, 'Bias': biases}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df = df.dropna()\n",
    "df.isnull().sum()\n",
    "\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Article</th>\n",
       "      <th>Bias</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>arizona republican gubernatori candid kari lak...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>washington former vice presid mike penc wednes...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>modern republican party’ one nonnegoti valu ru...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>editor’ note elizabeth alexand poet scholar pr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>editor’ note van jone host polit comment found...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2686</th>\n",
       "      <td>2686</td>\n",
       "      <td>amsterdam’ worldfam redlight district could lo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2687</th>\n",
       "      <td>2687</td>\n",
       "      <td>crime risen major citi nationwid recent year m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2688</th>\n",
       "      <td>2688</td>\n",
       "      <td>first nonprofit liber billionair georg soro op...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2689</th>\n",
       "      <td>2689</td>\n",
       "      <td>new listen news articles!twitt ceo elon musk g...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2690</th>\n",
       "      <td>2690</td>\n",
       "      <td>new blow embattl oberlin colleg ohio iranian n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2691 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                            Article  Bias\n",
       "0              0  arizona republican gubernatori candid kari lak...     0\n",
       "1              1  washington former vice presid mike penc wednes...     0\n",
       "2              2  modern republican party’ one nonnegoti valu ru...     0\n",
       "3              3  editor’ note elizabeth alexand poet scholar pr...     0\n",
       "4              4  editor’ note van jone host polit comment found...     0\n",
       "...          ...                                                ...   ...\n",
       "2686        2686  amsterdam’ worldfam redlight district could lo...     1\n",
       "2687        2687  crime risen major citi nationwid recent year m...     1\n",
       "2688        2688  first nonprofit liber billionair georg soro op...     1\n",
       "2689        2689  new listen news articles!twitt ceo elon musk g...     1\n",
       "2690        2690  new blow embattl oberlin colleg ohio iranian n...     1\n",
       "\n",
       "[2691 rows x 3 columns]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gf = pd.read_csv('finalDataset')\n",
    "gf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_160\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_158 (Embedding)   (None, 1500, 16)          640000    \n",
      "                                                                 \n",
      " global_average_pooling1d_98  (None, 16)               0         \n",
      "  (GlobalAveragePooling1D)                                       \n",
      "                                                                 \n",
      " dense_343 (Dense)           (None, 16)                272       \n",
      "                                                                 \n",
      " dense_344 (Dense)           (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 640,289\n",
      "Trainable params: 640,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/20\n",
      "68/68 [==============================] - 2s 13ms/step - loss: 0.6792 - accuracy: 0.6224 - val_loss: 0.6750 - val_accuracy: 0.5929\n",
      "Epoch 2/20\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.6525 - accuracy: 0.6359 - val_loss: 0.6650 - val_accuracy: 0.5929\n",
      "Epoch 3/20\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.6313 - accuracy: 0.6359 - val_loss: 0.6360 - val_accuracy: 0.5929\n",
      "Epoch 4/20\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.5950 - accuracy: 0.6405 - val_loss: 0.5883 - val_accuracy: 0.6617\n",
      "Epoch 5/20\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.5452 - accuracy: 0.7069 - val_loss: 0.5394 - val_accuracy: 0.7007\n",
      "Epoch 6/20\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.4941 - accuracy: 0.7738 - val_loss: 0.4938 - val_accuracy: 0.7286\n",
      "Epoch 7/20\n",
      "68/68 [==============================] - 1s 9ms/step - loss: 0.4423 - accuracy: 0.8119 - val_loss: 0.4390 - val_accuracy: 0.8197\n",
      "Epoch 8/20\n",
      "68/68 [==============================] - 1s 9ms/step - loss: 0.3924 - accuracy: 0.8518 - val_loss: 0.3839 - val_accuracy: 0.8625\n",
      "Epoch 9/20\n",
      "68/68 [==============================] - 1s 9ms/step - loss: 0.3435 - accuracy: 0.8788 - val_loss: 0.3331 - val_accuracy: 0.9071\n",
      "Epoch 10/20\n",
      "68/68 [==============================] - 1s 9ms/step - loss: 0.3029 - accuracy: 0.9025 - val_loss: 0.2912 - val_accuracy: 0.9312\n",
      "Epoch 11/20\n",
      "68/68 [==============================] - 1s 9ms/step - loss: 0.2612 - accuracy: 0.9285 - val_loss: 0.2563 - val_accuracy: 0.9461\n",
      "Epoch 12/20\n",
      "68/68 [==============================] - 1s 9ms/step - loss: 0.2294 - accuracy: 0.9419 - val_loss: 0.2271 - val_accuracy: 0.9628\n",
      "Epoch 13/20\n",
      "68/68 [==============================] - 1s 9ms/step - loss: 0.2063 - accuracy: 0.9424 - val_loss: 0.2027 - val_accuracy: 0.9647\n",
      "Epoch 14/20\n",
      "68/68 [==============================] - 1s 9ms/step - loss: 0.1802 - accuracy: 0.9577 - val_loss: 0.1821 - val_accuracy: 0.9647\n",
      "Epoch 15/20\n",
      "68/68 [==============================] - 1s 9ms/step - loss: 0.1640 - accuracy: 0.9591 - val_loss: 0.1698 - val_accuracy: 0.9480\n",
      "Epoch 16/20\n",
      "68/68 [==============================] - 1s 9ms/step - loss: 0.1486 - accuracy: 0.9670 - val_loss: 0.1507 - val_accuracy: 0.9665\n",
      "Epoch 17/20\n",
      "68/68 [==============================] - 1s 9ms/step - loss: 0.1341 - accuracy: 0.9638 - val_loss: 0.1420 - val_accuracy: 0.9703\n",
      "Epoch 18/20\n",
      "68/68 [==============================] - 1s 9ms/step - loss: 0.1243 - accuracy: 0.9670 - val_loss: 0.1299 - val_accuracy: 0.9721\n",
      "Epoch 19/20\n",
      "68/68 [==============================] - 1s 9ms/step - loss: 0.1154 - accuracy: 0.9689 - val_loss: 0.1291 - val_accuracy: 0.9610\n",
      "Epoch 20/20\n",
      "68/68 [==============================] - 1s 9ms/step - loss: 0.1074 - accuracy: 0.9735 - val_loss: 0.1132 - val_accuracy: 0.9758\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "[[6.6441089e-01]\n",
      " [9.5958626e-01]\n",
      " [7.8467596e-01]\n",
      " [4.1746745e-01]\n",
      " [8.6928004e-01]\n",
      " [3.3908978e-02]\n",
      " [1.3653630e-01]\n",
      " [1.0061364e-05]\n",
      " [7.4939929e-02]\n",
      " [9.5520532e-01]\n",
      " [6.0362939e-04]\n",
      " [4.0986156e-03]\n",
      " [1.5934242e-01]\n",
      " [3.4772556e-03]\n",
      " [5.7617433e-02]\n",
      " [1.1114038e-01]\n",
      " [9.8570102e-01]\n",
      " [4.4103363e-04]\n",
      " [4.7615854e-04]\n",
      " [9.8724014e-01]]\n",
      "2153    0\n",
      "2154    1\n",
      "2155    1\n",
      "2156    1\n",
      "2157    1\n",
      "2158    0\n",
      "2159    0\n",
      "2160    0\n",
      "2161    0\n",
      "2162    1\n",
      "2163    0\n",
      "2164    0\n",
      "2165    0\n",
      "2166    0\n",
      "2167    1\n",
      "2168    0\n",
      "2169    1\n",
      "2170    0\n",
      "2171    0\n",
      "2172    1\n",
      "Name: Bias, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\markv\\AppData\\Local\\Temp\\ipykernel_18576\\1079722132.py:92: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  print(test_label[:20])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.data_utils import pad_sequences\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.utils import compute_class_weight\n",
    "opt = tf.optimizers.Adam(learning_rate = 0.001)\n",
    "#opt = SGD(lr=0.00001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "\n",
    "\n",
    "#splitting data into a training and test dataset\n",
    "split = round(len(df)*0.8)\n",
    "train_reviews = df['Article'][:split]\n",
    "train_label = df['Bias'][:split]\n",
    "test_reviews = df['Article'][split:]\n",
    "test_label = df['Bias'][split:]\n",
    "\n",
    "#sanitizing all data as a string\n",
    "import numpy as np\n",
    "training_sentences = []\n",
    "training_labels = []\n",
    "testing_sentences = []\n",
    "testing_labels = []\n",
    "for row in train_reviews:\n",
    "    training_sentences.append(str(row))\n",
    "for row in train_label:\n",
    "    training_labels.append(int(row))\n",
    "for row in test_reviews:\n",
    "    testing_sentences.append(str(row))\n",
    "for row in test_label:\n",
    "    testing_labels.append(int(row))\n",
    "\n",
    "# variable constants\n",
    "vocab_size = 40000\n",
    "embedding_dim = 16\n",
    "max_length =  1500\n",
    "trunc_type = 'post'\n",
    "oov_tok = '<OOV>'\n",
    "padding_type = 'post'\n",
    "\n",
    "# tokenizing words\n",
    "#tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "    # sentences\n",
    "sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "padded = pad_sequences(sequences, maxlen=max_length, truncating=trunc_type)\n",
    "testing_sentences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "testing_padded = pad_sequences(testing_sentences, maxlen=max_length)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length), # usually essential/first layer\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    #tf.keras.layers.GlobalMaxPooling1D(),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    #tf.keras.layers.Dense(12, activation='tanh'),\n",
    "    #tf.keras.layers.Dense(12, activation='softmax'), # don't use softmax for output layer\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid'),\n",
    "    #tf.keras.layers.Dense(1, activation='softmax')\n",
    "])\n",
    "\n",
    "#model.add(BatchNormalization())\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "#training \n",
    "\n",
    "training_labels_final = np.array(training_labels)\n",
    "testing_labels_final = np.array(testing_labels)\n",
    "#class_weight = {1:0.5,1:1.}\n",
    "checkpoint1 = ModelCheckpoint(\"best_model1.hdf5\", monitor='val_accuracy', verbose=1,save_best_only=True, mode='auto', period=1,save_weights_only=False)\n",
    "\n",
    "num_epochs = 20\n",
    "#history = model.fit(padded, training_labels_final, epochs=num_epochs, validation_data=(testing_padded, testing_labels_final), shuffle=True, callbacks=[checkpoint1])\n",
    "model.fit(padded, training_labels_final, epochs=num_epochs, validation_data=(testing_padded, testing_labels_final), shuffle=True)\n",
    "\n",
    "prediction1 = model.predict(testing_padded[:20])\n",
    "print(prediction1)\n",
    "print(test_label[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "replacements = [\",\", \".\", \":\", '\\'','\\'', '\"', \"!\", \"?\", \".\" \";\", \"@\", \"{\", \"}\", \"[\", \"]\", \"(\", \")\", \"/\", \"#\", \"*\", \"_\", \"%\", \"-\", \"£\", \"$\", \"&\"]\n",
    "articles_prepositions = ['a', 'an', 'the', 'about', 'above', 'across', 'after', 'against', 'among', 'around',\n",
    "'at', 'before', 'behind', 'below', 'beside', 'between', 'by', 'down', 'during', 'for', 'from', 'in', 'inside',\n",
    "'into', 'near', 'of', 'off', 'on', 'out', 'over', 'through', 'to', 'toward', 'under', 'up', 'with']\n",
    "articles = []\n",
    "details = []\n",
    "biases = []\n",
    "for i in range(0,300):\n",
    "        words = []\n",
    "        counts = []\n",
    "        isPunctuation = []\n",
    "        articles.append('Article %d' % i)\n",
    "        # Opening the article file, assigning text it's contents, and closing the file\n",
    "        f = open(\"cleanedArticles/Article%d.txt\" % i, 'r', encoding=\"utf-8\")\n",
    "        lines = f.readlines()\n",
    "        f.close()\n",
    "\n",
    "# reading the sanitized data from Article0 and turning it into a pandas dataframe\n",
    "        for line in lines:\n",
    "                data = line.split()\n",
    "                newWord = data[0]\n",
    "                newWord = newWord[0:(len(newWord)-1)]\n",
    "                \n",
    "                for exempt in articles_prepositions:\n",
    "                        if(newWord==exempt):\n",
    "                                continue\n",
    "\n",
    "                if((len(newWord)==3) and (newWord[1:2] in letters)):\n",
    "                        continue\n",
    "                for r in range(0, len(replacements)):\n",
    "                        if(len(newWord) > 3): \n",
    "                                newWord = newWord.replace(replacements[r], \"\")\n",
    "                        \n",
    "                for r in range(0, len(replacements)):\n",
    "                        data[1] = data[1].replace(replacements[r], \"\")\n",
    "                if(len(newWord)>2):\n",
    "                        if(newWord[(len(newWord)-1):(len(newWord))] == 'i'):\n",
    "                                newWord = newWord[0:(len(newWord)-1)]\n",
    "\n",
    "                words.append(newWord)\n",
    "                counts.append(data[1])\n",
    "        g=0\n",
    "        #implementing the isPunctuation functionality\n",
    "        for word in words:\n",
    "                isPunctuation.append('')\n",
    "\n",
    "                if(len(word) == 3):\n",
    "                        for letter in letters:\n",
    "                                if(word.__contains__(letter)):\n",
    "                                        isPunctuation[g] = ('False')\n",
    "                        if(isPunctuation[g] != 'False'):\n",
    "                                isPunctuation[g] = 'True'\n",
    "                if(isPunctuation[g] == ''):\n",
    "                        isPunctuation[g] = 'False'\n",
    "                g=g+1\n",
    "        newWords = []\n",
    "        g=0\n",
    "        for word in words:\n",
    "                if(isPunctuation[g] == 'True'):\n",
    "                        word = word[1:2]\n",
    "                newWords.append(word)\n",
    "                g+=1\n",
    "        \n",
    "        # storing every sentence in an article as a list\n",
    "        sentences = []\n",
    "        text = \"\"\n",
    "\n",
    "        f = open(\"articles/article%d.txt\" % i, 'r', encoding=\"utf-8\")\n",
    "        text = str(f.read()).lower()\n",
    "        f.close()\n",
    "        closingPunctuation = ['.', '?', '!']\n",
    "\n",
    "        text = text.replace(\"\\n\", \"\")\n",
    "\n",
    "        for r in range(0, len(closingPunctuation)):\n",
    "            text = text.replace((closingPunctuation[r]), closingPunctuation[r] + \"|\")\n",
    "            text = text.replace(\"| \", \"|\")\n",
    "\n",
    "        sentences = text.split('|')\n",
    "\n",
    "        mapping = []\n",
    "        map = {}\n",
    "        for word in newWords:\n",
    "                sentenceIndex=0\n",
    "                sentenceList = \"\"\n",
    "                for sentence in sentences:\n",
    "                        if(sentence.__contains__(word)):\n",
    "                                sentenceList = sentenceList + str(sentenceIndex) + \" \"\n",
    "                        sentenceIndex+=1\n",
    "                sentenceList = sentenceList[0:(len(sentenceList)-1)]\n",
    "                mapping.append(sentenceList)\n",
    "\n",
    "        #finding words that are used in the same sentences\n",
    "        sentences = []\n",
    "        text = \"\"\n",
    "        f = open(\"articles/article0.txt\", 'r', encoding=\"utf-8\")\n",
    "        text = str(f.read()).lower()\n",
    "        f.close()\n",
    "        closingPunctuation = ['.', '?', '!']\n",
    "\n",
    "        text = text.replace(\"\\n\", \"\")\n",
    "\n",
    "        for r in range(0, len(closingPunctuation)):\n",
    "                text = text.replace((closingPunctuation[r]), closingPunctuation[r] + \"|\")\n",
    "                text = text.replace(\"| \", \"|\")\n",
    "\n",
    "        sentences = text.split('|')\n",
    "\n",
    "\n",
    "        # it works!\n",
    "\n",
    "        deets = []\n",
    "        terms = []\n",
    "        sentenceGroups = []\n",
    "        sentenceInfo =[]\n",
    "        terms = newWords\n",
    "        #logic here is a little sloppy, could be condensed\n",
    "        for term in terms:\n",
    "                sentenceGroups=[]\n",
    "                sentenceID = 0\n",
    "                for sentence in sentences:\n",
    "                        wordsInSentence= []\n",
    "                        wordsTogether = []\n",
    "                        if(sentence.__contains__(term)):\n",
    "                                wordsInSentence = sentence.split()\n",
    "                                for potentialWord in wordsInSentence:\n",
    "                                        if(potentialWord!=term):\n",
    "                                                for replacee in replacements:\n",
    "                                                        potentialWord = potentialWord.replace(replacee, '')\n",
    "                                                wordsTogether.append(potentialWord)\n",
    "                        else:\n",
    "                                continue\n",
    "                        sentenceGroups.append(wordsTogether)\n",
    "                sentenceInfo.append(sentenceGroups)\n",
    "        g = open(\"articleBiases/article%dBias.txt\" % i, 'r', encoding=\"utf-8\")\n",
    "        bias = g.read()\n",
    "        g.close()\n",
    "        if(bias.__contains__('-1')):\n",
    "                biases.append('-1')\n",
    "        else:\n",
    "                biases.append('1')\n",
    "        \n",
    "        details.append({'Words': newWords, 'Counts':counts, 'isPunctuation': isPunctuation, 'sentenceMap': mapping, 'Associations': sentenceInfo})\n",
    "        \n",
    "dfnew = pd.DataFrame(details)\n",
    "#dfnew['Article'] = articles\n",
    "dfnew['Bias'] = biases\n",
    "\n",
    "dfnew"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ffb1946c53a73a16fb17cf34a21078f622b7ab4e4f0efabd45e069bd72d86fa8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
